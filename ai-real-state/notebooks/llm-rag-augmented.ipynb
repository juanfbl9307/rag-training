{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T00:16:43.662629Z",
     "start_time": "2025-02-12T00:16:40.417501Z"
    }
   },
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.5, model=model, max_tokens=4096)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "from langchain_chroma import Chroma\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"data-chunking-pdf\"\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "print(chroma_client.heartbeat())\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"llm-rag-ai\",\n",
    "    embedding_function=embeddings,\n",
    "    client=chroma_client\n",
    ")\n",
    "retriever = vector_store.as_retriever()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1739319403649052376\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T00:16:44.831271Z",
     "start_time": "2025-02-12T00:16:44.826321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "template = \"\"\"\n",
    "**Context**: {context}\n",
    "You are an AI assistant specialized in answering questions about Artificial Intelligence (AI), Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Python programming. Your task is to provide clear, accurate, and concise explanations. If the question is outside your scope, politely inform the user.\n",
    "\n",
    "**User Question**: {question}\n",
    "\n",
    "**Your Response**:\n",
    "1. Provide a clear and structured answer.\n",
    "2. Include examples, code snippets, or analogies if applicable.\n",
    "3. Do not include any additional explanations or context.\n",
    "4. Do not hallucinate.\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", template),\n",
    "    (\"user\", \"{question}\")\n",
    "])"
   ],
   "id": "1e4fcf5ba340e3b2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T00:16:47.130148Z",
     "start_time": "2025-02-12T00:16:44.843203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke(\"give me some use cases for rag\")\n",
    "print(response)"
   ],
   "id": "87071815fe0b7f25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some use cases for Retrieval-Augmented Generation (RAG) include:\n",
      "\n",
      "1. **Healthcare**: Generating personalized treatment plans for patients based on their medical history and current health status.\n",
      "2. **Finance**: Providing investment advice based on market trends and client preferences.\n",
      "3. **Education**: Generating personalized educational content for students based on their learning needs.\n",
      "4. **Real Estate**: Analyzing housing prices and demographic information to provide accurate property valuations.\n",
      "5. **Energy**: Optimizing energy supply and reducing waste in the energy sector.\n",
      "\n",
      "These are just a few examples of how RAG can be applied across various industries to generate personalized answers and recommendations.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T00:16:47.356383Z",
     "start_time": "2025-02-12T00:16:47.143553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"],k=3)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt_template.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ],
   "id": "af6ed2a1aebf98db",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T00:20:42.946771Z",
     "start_time": "2025-02-12T00:20:40.781745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = graph.invoke({\"question\": \"explain langchain\"})\n",
    "print(response[\"answer\"])"
   ],
   "id": "a3bd2512d0288d81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by language models. It enables context-aware applications that connect a language model to various sources of context and allows for reasoning based on the provided context. LangChain provides a digital toolbox to build intelligent applications that can communicate, understand, and make decisions similar to humans. It allows developers to tap into the knowledge of advanced language models like GPT-4, PaLM, Gemini, and open source models such as LLaMA. Additionally, LangChain chains are foundational concepts that help build robust generative AI applications by organizing tasks into interconnected steps within an efficient workflow.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
